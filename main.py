# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import pandas as pd
from  summarizer  import  Summarizer

import nltk
from nltk.tokenize import sent_tokenize
from nltk.probability import FreqDist
from nltk.translate.bleu_score import corpus_bleu
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction
from bert_score import score

from collections import defaultdict
from rouge import Rouge

import json
import torch

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score

# –°—á–∏—Ç—ã–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞
with open('gazeta_train.jsonl', 'r', encoding='utf-8') as file:
    train = file.readlines()
train_json = [json.loads(line) for line in train]
train_data = pd.DataFrame(train_json)

# –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å—Ç–æ–ª–±—Ü—ã
del train_data['date']
del train_data['url']
del train_data['title']

# –≠–º–æ–¥–∑–∏ –∏ –∏—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
emoji_dict = {
    "ü§£": "—Å–º–µ—à–Ω–æ",
    # –í –¥–∞–ª—å–Ω–µ–π—à–µ–º —Å–ø–∏—Å–æ–∫ –±—É–¥–µ—Ç –ø–æ–ø–æ–ª–Ω—è—Ç—å—Å—è
}


# –û–ø—Ä–µ–¥–µ–ª–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–æ–±—â–µ–Ω–∏–π

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–Ω–∏—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
train = pd.read_csv('train.csv')

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Å –ø–æ–º–æ—â—å—é TF-IDF
vectorizer = TfidfVectorizer(max_features=100000)
X_train_tfidf = vectorizer.fit_transform(train['text'])
y_train = train['sentiment']

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ SVM
svm_model = LinearSVC()
svm_model.fit(X_train_tfidf, y_train)

# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –≤ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Å –ø–æ–º–æ—â—å—é TF-IDF
X_test_tfidf = vectorizer.transform(train_data['text'])

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
sentiments = svm_model.predict(X_test_tfidf)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –≤ –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü –≤ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
train_data['sentiment'] = sentiments


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
def extractive_summarization(text):
    sentences = sent_tokenize(text)

    # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Å–ª–æ–≤
    words = [word for sentence in sentences for word in nltk.word_tokenize(sentence.lower()) if word.isalnum()]
    freq = FreqDist(words)

    ranking = defaultdict(int)

    for i, sentence in enumerate(sentences):
        for word in nltk.word_tokenize(sentence.lower()):
            if word in freq:
                ranking[i] += freq[word]

    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É
    ranked_sentences = sorted(((rank, i) for i, rank in ranking.items()), reverse=True)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
    num_sentences = int(len(sentences) * 0.5)

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ø –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    summary = ' '.join(sentences[idx] for _, idx in ranked_sentences[:num_sentences])

    return summary

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
abstractive_summarization = Summarizer()

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —ç–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞, –∞ –∑–∞—Ç–µ–º –∞–±—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –∫–∞–∂–¥—ã–π —Ç–µ–∫—Å—Ç
predicted_summaries = []
for text in train_data['text']:

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —ç–º–æ–¥–∑–∏ –≤ —Ç–µ–∫—Å—Ç
    for emoji_char, text_representation in emoji_dict.items():
        text = text.replace(emoji_char, text_representation)

    # –≠–∫—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
    extractive_summary = extractive_summarization(text)
    # –ê–±—Å—Ç—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
    abstractive_summary = abstractive_summarization(text, num_sentences=2)
    result = ''.join(abstractive_summary)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –±—É–¥—É—â–∏–π —Å—Ç–æ–ª–±–µ—Ü
    predicted_summaries.append(result)

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –Ω–æ–≤—ã–π —Å—Ç–æ–ª–±–µ—Ü 'predicted'
train_data['predicted'] = predicted_summaries

# –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω—ã—Ö –∫—Ä–∞—Ç–∫–∏—Ö –æ–ø–∏—Å–∞–Ω–∏–π –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏ Bert score
references = train_data['summary'].tolist()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä–∞ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç—Ä–∏–∫–∏ Bert score
bert_scores = []
for i in range(len(predicted_summaries)):
    score_result = score([predicted_summaries[i]], [references[i]], lang='ru')
    bert_scores.append(score_result)

# –ü–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π 'predicted' –∏ 'summary'
bleu_scores = []
mean_rouge_l_scores = []

for index, row in train_data.iterrows():
    predicted_summary = row['predicted']
    reference_summary = row['summary']

    # BLEU score
    smoothing_function = SmoothingFunction().method1
    bleu_score = sentence_bleu([reference_summary.split()], predicted_summary.split(),
                               smoothing_function=smoothing_function)
    bleu_scores.append(bleu_score)

    # Rouge-L score
    rouge = Rouge()
    rouge_scores = rouge.get_scores(predicted_summary, reference_summary, avg=True)
    mean_rouge_l_scores.append(rouge_scores['rouge-l']['f'])

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫
mean_bleu = sum(bleu_scores) / len(bleu_scores)
mean_rouge_l = sum(mean_rouge_l_scores) / len(mean_rouge_l_scores)
bert_score = sum([score[1].item() for score in bert_scores]) / len(bert_scores)

# –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫
print("–†–µ–∑—É–ª—å—Ç–∞—Ç –º–µ—Ç—Ä–∏–∫–∏ Mean BLEU Score:", mean_bleu) # 0.08
print("–†–µ–∑—É–ª—å—Ç–∞—Ç –º–µ—Ç—Ä–∏–∫–∏ Mean Rouge-L Score:", mean_rouge_l) # 0.23
print("–†–µ–∑—É–ª—å—Ç–∞—Ç –º–µ—Ç—Ä–∏–∫–∏ Bert score:", bert_score) # 0.76
